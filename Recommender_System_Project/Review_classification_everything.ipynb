{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######importing libraries for data manipulation#######\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import math\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "from spark_sklearn import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import interp\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import mord\n",
    "from time import time\n",
    "import matplotlib.patches as mpatches\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############Function definitions###########\n",
    "\n",
    "#########Function definitions for separating related feature########\n",
    "def change_vals_new_col(s,value,new_cols):\n",
    "    \n",
    "    if(value.get(s) != None):\n",
    "        if((type(value[s]) == float) and np.isnan(value[s])):\n",
    "            new_cols[s] = np.nan\n",
    "        else:\n",
    "            new_cols.get(s).append(value[s])\n",
    "    else:\n",
    "        new_cols.get(s).append(np.nan)\n",
    "    \n",
    "\n",
    "def generate_new_cols(related):\n",
    "    \n",
    "    new_cols = {'also_bought':[], 'also_viewed':[],'bought_together':[],'buy_after_viewing':[]}\n",
    "    \n",
    "    for key,value in related.items():\n",
    "        if((type(value) == float) and np.isnan(value)):\n",
    "            \n",
    "            new_cols['also_bought'].append(np.nan)\n",
    "            new_cols['also_viewed'].append(np.nan)\n",
    "            new_cols['bought_together'].append(np.nan)\n",
    "            new_cols['buy_after_viewing'].append(np.nan)\n",
    "        else:\n",
    "            change_vals_new_col('also_bought',value,new_cols)\n",
    "            change_vals_new_col('also_viewed',value,new_cols)\n",
    "            change_vals_new_col('bought_together',value,new_cols)\n",
    "            change_vals_new_col('buy_after_viewing',value,new_cols)\n",
    "            \n",
    "        \n",
    "    return new_cols\n",
    "#####Function definitions for separating related feature ends####\n",
    "\n",
    "def plot_related_prods(related,which,final_metadata):\n",
    "    \n",
    "    if(related == None):\n",
    "        print('People who'+str(related)+'this product did not buy any other product:')\n",
    "        return\n",
    "    else:\n",
    "        #print(np.array(related) in final_metadata.index)\n",
    "        tot = 0\n",
    "        for idx in related:\n",
    "            if(idx in final_metadata.index):\n",
    "                tot += 1\n",
    "        print(tot)\n",
    "        tot = round(tot/2)\n",
    "        print('final',tot)\n",
    "        f, axes = plt.subplots(tot,tot,figsize=(4,4),dpi=300)\n",
    "        f.suptitle('People also '+str(which))\n",
    "        for i in range(0,tot):\n",
    "            for j in range(0,tot):\n",
    "                curr_asin = related[i+j]\n",
    "                if((curr_asin in final_metadata.index) == True):\n",
    "                    curr_url = final_metadata.loc[curr_asin]['imUrl']\n",
    "                    curr_title = final_metadata.loc[curr_asin]['title']\n",
    "                    curr_title = curr_title[0:30]\n",
    "                    response = requests.get(curr_url)\n",
    "                    img = Image.open(BytesIO(response.content))\n",
    "                    axes[i,j].imshow(img)\n",
    "                    axes[i,j].get_xaxis().set_ticks([])\n",
    "                    axes[i,j].get_yaxis().set_ticks([])\n",
    "                    plt.axis('off')\n",
    "                    axes[i,j].set_title(curr_title,size=3)\n",
    "        plt.show()            \n",
    "\n",
    "def Show_related_products(meta_data_row,final_metadata):\n",
    "    \n",
    "    #print(meta_data_row)\n",
    "    curr_url = meta_data_row['imUrl']\n",
    "    #curr_prod_id = meta_data_row['asin']\n",
    "    title = meta_data_row['title']\n",
    "    \n",
    "    print('The current product is:',title)\n",
    "    response = requests.get(curr_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    ####People who bought this product also bought####\n",
    "    also_bought = meta_data_row['also_bought']\n",
    "    if((type(also_bought) == float) and np.isnan(also_bought)):\n",
    "        also_bought = None\n",
    "    else:\n",
    "        if(len(also_bought) > 9):\n",
    "            also_bought = also_bought[0:9]\n",
    "    plot_related_prods(also_bought,'bought',final_metadata)\n",
    "    \n",
    "    ####People who bought this product also viewed####\n",
    "    also_viewed = meta_data_row['also_viewed']\n",
    "    if((type(also_viewed) == float) and np.isnan(also_viewed)):\n",
    "        also_viewed = None\n",
    "    else:\n",
    "        if(len(also_viewed) > 9):\n",
    "            also_viewed = also_viewed[0:9]\n",
    "    plot_related_prods(also_viewed,'viewed',final_metadata)\n",
    "\n",
    "\n",
    "def bootStrap(learner,data,Y,size):\n",
    "    \n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    train_f1 = []\n",
    "    test_f1 = []\n",
    "    data_merged = pd.concat([data,Y],axis=1)\n",
    "    for i in range(0,size):\n",
    "        #print(i)\n",
    "        data_sampled = data_merged.sample(5000)\n",
    "        \n",
    "        X = data_sampled.iloc[:,0:(data_sampled.shape[1]-1)]\n",
    "        Y = data_sampled.iloc[:,(data_sampled.shape[1]-1):data_sampled.shape[1]]\n",
    "        \n",
    "        #print(X.shape)\n",
    "        #print(Y.shape)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "        \n",
    "        #print(X_train.shape)\n",
    "        #print(y_train.shape)\n",
    "        \n",
    "        learner = learner.fit(X_train,y_train)\n",
    "        \n",
    "        predictions_test = learner.predict(X_test)\n",
    "        predictions_train = learner.predict(X_train)\n",
    "        \n",
    "        train_acc.append(accuracy_score(y_train,predictions_train))\n",
    "        test_acc.append(accuracy_score(y_test,predictions_test))\n",
    "        train_f1.append(f1_score(y_train,predictions_train))\n",
    "        test_f1.append(f1_score(y_test,predictions_test))\n",
    "        \n",
    "    \n",
    "    return train_acc,test_acc,train_f1,test_f1\n",
    "\n",
    "def Hypothesis_test(sampling_dist,s1):\n",
    "    \n",
    "    train_acc_samp = sampling_dist[0] \n",
    "    test_acc_samp = sampling_dist[1]\n",
    "    train_f1_samp = sampling_dist[2]\n",
    "    test_f1_samp = sampling_dist[3]\n",
    "\n",
    "    train_acc_mean = np.mean(train_acc_samp)\n",
    "    train_acc_std = np.std(train_acc_samp)\n",
    "    train_acc_SE = train_acc_std/np.sqrt(100) \n",
    "    print('Train accuracy mean of ',s1,' is',train_acc_mean)\n",
    "    print('Train accuracy standard deviation of ',s1,' is',train_acc_std)\n",
    "    print('Train accuracy Standard error of ',s1,' is',train_acc_SE)\n",
    "    print('-----------------------------------------------------------')\n",
    "    print('-----------------------------------------------------------')\n",
    "\n",
    "    test_acc_mean = np.mean(test_acc_samp)\n",
    "    test_acc_std = np.std(test_acc_samp)\n",
    "    test_acc_SE = test_acc_std/np.sqrt(100)\n",
    "    print('Test accuracy mean of ',s1,' is: ',test_acc_mean)\n",
    "    print('Test accuracy standard deviation of ',s1,' is',test_acc_std)\n",
    "    print('Test accuracy Standard error of ',s1,' is',test_acc_SE)\n",
    "    print('-----------------------------------------------------------')\n",
    "    print('-----------------------------------------------------------')\n",
    "\n",
    "    train_f1_mean = np.mean(train_f1_samp)\n",
    "    train_f1_std = np.std(train_f1_samp)\n",
    "    train_f1_SE = train_f1_std/np.sqrt(100)\n",
    "    print('Train f1 mean of ',s1,' is',train_f1_mean)\n",
    "    print('Train f1 standard deviation of ',s1,' is',train_f1_std)\n",
    "    print('Train f1 Standard error of ',s1,' is',train_f1_SE)\n",
    "    print('-----------------------------------------------------------')\n",
    "    print('-----------------------------------------------------------')\n",
    "\n",
    "    test_f1_mean = np.mean(test_f1_samp)\n",
    "    test_f1_std = np.std(test_f1_samp)\n",
    "    test_f1_SE = test_f1_std/np.sqrt(100)\n",
    "    print('Test f1 mean of ',s1,' is',test_f1_mean)\n",
    "    print('Test f1 standard deviation of ',s1,' is',test_f1_std)\n",
    "    print('Test f1 Standard error of ',s1,' is',test_f1_SE)\n",
    "    print('-----------------------------------------------------------')\n",
    "    print('-----------------------------------------------------------')\n",
    "    \n",
    "    dist = np.random.normal(loc=test_acc_mean,scale=test_acc_SE,size = 10000)\n",
    "    density_prop = {\"color\": \"green\"}\n",
    "    hist_prop = {\"alpha\": 0.3, \"color\": \"red\"}\n",
    "    s = '95 % confidence interval of test accuracy of '+s1\n",
    "    plot_densityCurve(dist,density_prop,hist_prop,100,5000,test_acc_mean,test_acc_SE,accuracy_naive,s)\n",
    "    \n",
    "    dist = np.random.normal(loc=test_f1_mean,scale=test_f1_SE,size = 10000)\n",
    "    density_prop = {\"color\": \"green\"}\n",
    "    hist_prop = {\"alpha\": 0.3, \"color\": \"red\"}\n",
    "    s = '95 % confidence interval of test f_beta(0.5) score '+s1\n",
    "    plot_densityCurve(dist,density_prop,hist_prop,100,5000,test_f1_mean,test_f1_SE,fscore_naive,s1)\n",
    "    \n",
    "    \n",
    "    \n",
    "def f_beta(y_true,y_pred,beta):\n",
    "    \n",
    "    precision = precision_score(y_true,y_pred)\n",
    "    recall = recall_score(y_true,y_pred)\n",
    "\n",
    "    # TODO: Calculate F-score using the formula above for beta = 0.5 and correct values for precision and recall.\n",
    "    fscore = ((1+beta*beta)*(precision*recall))/((beta*beta*precision)+recall)\n",
    "    \n",
    "    return fscore\n",
    "    \n",
    "\n",
    "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    X_train = X_train.astype(int)\n",
    "    y_train = y_train.astype(int)\n",
    "    X_test = X_test.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    results = {}\n",
    "    \n",
    "    # TODO: Fit the learner to the training data using slicing with 'sample_size' using .fit(training_features[:], training_labels[:])\n",
    "    start = time() # Get start time\n",
    "    learner = learner.fit(X_train[0:sample_size],y_train[0:sample_size])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the training time\n",
    "    results['train_time'] = end-start\n",
    "        \n",
    "    # TODO: Get the predictions on the test set(X_test),\n",
    "    #       then get predictions on the first 300 training samples(X_train) using .predict()\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train[0:sample_size])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the total prediction time\n",
    "    results['pred_time'] = end - start\n",
    "            \n",
    "    # TODO: Compute accuracy on the first 300 training samples which is y_train[:300]\n",
    "    results['acc_train'] = accuracy_score(y_train[0:sample_size],predictions_train)\n",
    "        \n",
    "    # TODO: Compute accuracy on test set using accuracy_score()\n",
    "    results['acc_test'] = accuracy_score(y_test,predictions_test)\n",
    "    \n",
    "    # TODO: Compute F-score on the the first 300 training samples using fbeta_score()\n",
    "\n",
    "    results['f_train'] = f_beta(y_train[0:sample_size],predictions_train,0.5)\n",
    "        \n",
    "    # TODO: Compute F-score on the test set which is y_test\n",
    "    results['f_test'] = f_beta(y_test,predictions_test,0.5)\n",
    "       \n",
    "    # Success\n",
    "    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
    "    print(\"Train accuracy is:\", results['acc_train'])\n",
    "    print(\"Test accuracy is:\", results['acc_test'])\n",
    "    print(\"Train F-beta(0.5) score is:\", results['f_train'])\n",
    "    print(\"Test F-beta(0.5) is:\", results['f_test'])\n",
    "    print('_________________________________________')\n",
    "    print('_________________________________________')\n",
    "        \n",
    "    # Return the results\n",
    "    return results\n",
    "\n",
    "def Naive_Bayes(X,Y,which,avg='binary'):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=42)\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred_train = clf.predict(X_train)\n",
    "    print('Train accuracy score of ',which,' NB is: ',accuracy_score(y_train, pred_train, normalize=True))\n",
    "    print('Train F-1 ',which,' NB is: ',f1_score(y_train, pred_train,average=avg))\n",
    "    pred_test = clf.predict(X_test)\n",
    "    print('Test accuracy score of ',which,' NB is: ',accuracy_score(y_test, pred_test, normalize=True))\n",
    "    print('Test F-1 ',which,' NB is: ',f1_score(y_test, pred_test,average=avg))\n",
    "    \n",
    "def SVM(X,Y,which,avg = 'binary'):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=42)\n",
    "    clf = SVC(kernel='rbf',C=2,gamma=20)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred_train = clf.predict(X_train)\n",
    "    print('Train accuracy score of ',which,' SVM is: ',accuracy_score(y_train, pred_train, normalize=True))\n",
    "    print('Train F-1 ',which,' SVM is: ',f1_score(y_train, pred_train,average=avg))\n",
    "    pred_test = clf.predict(X_test)\n",
    "    print('Test accuracy score of ',which,' SVM is: ',accuracy_score(y_test, pred_test, normalize=True))\n",
    "    print('Test F-1 ',which,' SVM is: ',f1_score(y_test, pred_test,average=avg))\n",
    "    \n",
    "    \n",
    "def Perceptron_classifier(X,Y,which,weights=None,avg='binary'):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=42)\n",
    "    \n",
    "    clf = Perceptron(max_iter=200,class_weight=weights)\n",
    "    clf.fit(X_train,y_train)\n",
    "    pred_train = clf.predict(X_train)\n",
    "    \n",
    "    print('Train accuracy score of ',which,' Perceptron: ',accuracy_score(y_train, pred_train, normalize=True))\n",
    "    print('Train F-1 ',which,' Perceptron is: ',f1_score(y_train, pred_train,average=avg))\n",
    "    pred_test = clf.predict(X_test)\n",
    "    print('Test accuracy score of ',which,' Perceptron is: ',accuracy_score(y_test, pred_test, normalize=True))\n",
    "    print('Test F-1 ',which,' Perceptron is: ',f1_score(y_test, pred_test,average=avg))\n",
    "    \n",
    "def Random_Forest(X,Y,which,avg='binary'):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=42)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=200, max_depth=2,random_state=0)\n",
    "    clf.fit(X_train,y_train)\n",
    "    pred_train = clf.predict(X_train)\n",
    "    \n",
    "    print('Train accuracy score of ',which,' RandomForest: ',accuracy_score(y_train, pred_train, normalize=True))\n",
    "    print('Train F-1 ',which,' RandomForest is: ',f1_score(y_train, pred_train,average=avg))\n",
    "    pred_test = clf.predict(X_test)\n",
    "    print('Test accuracy score of ',which,' RandomForest is: ',accuracy_score(y_test, pred_test, normalize=True))\n",
    "    print('Test F-1 ',which,' RandomForest is: ',f1_score(y_test, pred_test,average=avg))\n",
    "    \n",
    "def Ada_Boost(X,Y,which,avg='binary'):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=42)\n",
    "    \n",
    "    clf = AdaBoostClassifier(n_estimators=200)\n",
    "    clf.fit(X_train,y_train)\n",
    "    pred_train = clf.predict(X_train)\n",
    "    \n",
    "    print('Train accuracy score of ',which,' RandomForest: ',accuracy_score(y_train, pred_train, normalize=True))\n",
    "    print('Train F-1 ',which,' RandomForest is: ',f1_score(y_train, pred_train,average=avg))\n",
    "    pred_test = clf.predict(X_test)\n",
    "    print('Test accuracy score of ',which,' RandomForest is: ',accuracy_score(y_test, pred_test, normalize=True))\n",
    "    print('Test F-1 ',which,' RandomForest is: ',f1_score(y_test, pred_test,average=avg))\n",
    "    \n",
    "def Ordinal_Logistic_regression(X,Y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = X_train.astype(int)\n",
    "    y_train = y_train.astype(int)\n",
    "    X_test = X_test.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    \n",
    "    clf = mord.LogisticIT()\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    pred_train = clf.predict(X_train)\n",
    "    #pred_train = np.round(pred_train,decimals=0)\n",
    "    #pred_train.astype(int)\n",
    "    print(pred_train[0:20])\n",
    "    print(Y[0:20])\n",
    "    print('Train accuracy score of multiclass ordinl Ridge regression is: ',accuracy_score(y_train, pred_train, normalize=True))\n",
    "    print('Train F-1 of multiclass ordinl Ridge regression is: ',f1_score(y_train, pred_train,average='weighted'))\n",
    "    \n",
    "    pred_test = clf.predict(X_test)\n",
    "    #pred_test = np.round(pred_test,decimals=0)\n",
    "    #pred_test.astype(int)\n",
    "    print('Test accuracy score of multiclass ordinl Ridge regression is: ',accuracy_score(y_train, pred_train, normalize=True))\n",
    "    print('Test F-1 of multiclass ordinl Ridge regression is: ',f1_score(y_train, pred_train,average='weighted'))\n",
    "\n",
    "#######Reciever Operating Characteristics definitions###########\n",
    "\n",
    "    \n",
    "def ROC_AUC(classifier,X,y,which=None,c=None):\n",
    "  \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    y = np.reshape(y,(y.shape[0],1))\n",
    "    title = 'ROC for Binary labels'\n",
    "    \n",
    "    if(which == 'multi'):\n",
    "        y = label_binarize(y, classes=[1,2,3,4,5])\n",
    "        title = 'ROC for multi-class labels'\n",
    "        #f, axes = plt.subplots(3,2,figsize=(8,8),dpi=300)\n",
    "        #f.suptitle('Distribution of drawing cards simulations')\n",
    "        #m = [(0,0),(0,1),(1,0),(1,1),(2,0),(2,1)]\n",
    "    else:\n",
    "        title = 'ROC for Binary labels'\n",
    "        #f, axes = plt.subplots(1,2,figsize=(8,8),dpi=300)\n",
    "        #f.suptitle('Distribution of drawing cards simulations')\n",
    "        #m = [(0,0),(0,1)]\n",
    "        \n",
    "        \n",
    "    print(y.shape)\n",
    "    random_state = np.random.RandomState(0)\n",
    "    cv = StratifiedKFold(n_splits=6)\n",
    "    \n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "    for t in range(0,y.shape[1]):\n",
    "        i = 0\n",
    "        tprs = []\n",
    "        aucs = []\n",
    "        #a = m[t][0]\n",
    "        #b = m[t][1]\n",
    "        #print(a)\n",
    "        #print(b)\n",
    "        for train, test in cv.split(X, y[:,t]):\n",
    "            #print(y[train].shape)\n",
    "            \n",
    "            if(c == 'p'):\n",
    "                probas_ = classifier.fit(X[train], y[train,t]).score(X[test],y[test,t])\n",
    "            else:\n",
    "                probas_ = classifier.fit(X[train], y[train,t]).predict_proba(X[test])\n",
    "            # Compute ROC curve and area the curve\n",
    "            \n",
    "            fpr, tpr, thresholds = roc_curve(y[test,t], probas_[:, 1])\n",
    "            tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "            tprs[-1][0] = 0.0\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            plt.plot(fpr, tpr, lw=1, alpha=0.3,label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))        \n",
    "            i = i + 1\n",
    "   \n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Chance', alpha=.8)\n",
    "        mean_tpr = np.mean(tprs, axis=0)\n",
    "        mean_tpr[-1] = 1.0\n",
    "        mean_auc = auc(mean_fpr, mean_tpr)\n",
    "        std_auc = np.std(aucs)\n",
    "        plt.plot(mean_fpr, mean_tpr, color='b',label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),lw=2, alpha=.8)\n",
    "               \n",
    "        std_tpr = np.std(tprs, axis=0)\n",
    "        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "        plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,label=r'$\\pm$ 1 std. dev.')\n",
    "               \n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        if(which == 'multi'):\n",
    "            plt.title(title+' for rating = '+str(t+1))\n",
    "        else:\n",
    "            plt.title(title)\n",
    "        \n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "def evaluate(results, accuracy, f1):\n",
    "    \"\"\"\n",
    "    Visualization code to display results of various learners.\n",
    "    \n",
    "    inputs:\n",
    "      - learners: a list of supervised learners\n",
    "      - stats: a list of dictionaries of the statistic results from 'train_predict()'\n",
    "      - accuracy: The score for the naive predictor\n",
    "      - f1: The score for the naive predictor\n",
    "    \"\"\"\n",
    "  \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(2, 3, figsize = (30,20))\n",
    "\n",
    "    # Constants\n",
    "    bar_width = 0.15\n",
    "    colors = ['#A00000','#00A0A0','#00A000','orange','purple']\n",
    "    \n",
    "    # Super loop to plot four panels of data\n",
    "    for k, learner in enumerate(results.keys()):\n",
    "        for j, metric in enumerate(['train_time', 'acc_train', 'f_train', 'pred_time', 'acc_test', 'f_test']):\n",
    "            for i in np.arange(3):\n",
    "                # Creative plot code\n",
    "                ax[j//3, j%3].bar(i+k*bar_width, results[learner][i][metric], width = bar_width, color = colors[k])\n",
    "                ax[j//3, j%3].set_xticks([0.45, 1.45, 2.45])\n",
    "                ax[j//3, j%3].set_xticklabels([\"1%\", \"10%\", \"100%\"])\n",
    "                ax[j//3, j%3].set_xlabel(\"Training Set Size\",fontsize = 26)\n",
    "                ax[j//3, j%3].set_xlim((-0.1, 3.0))\n",
    "    \n",
    "    # Add unique y-labels\n",
    "    ax[0, 0].set_ylabel(\"Time (in seconds)\",fontsize = 26)\n",
    "    ax[0, 1].set_ylabel(\"Accuracy Score\",fontsize = 26)\n",
    "    ax[0, 2].set_ylabel(\"F-beta(0.5)\",fontsize = 26)\n",
    "    ax[1, 0].set_ylabel(\"Time (in seconds)\",fontsize = 26)\n",
    "    ax[1, 1].set_ylabel(\"Accuracy Score\",fontsize = 26)\n",
    "    ax[1, 2].set_ylabel(\"F-beta(0.5)\",fontsize = 26)\n",
    "    \n",
    "    # Add titles\n",
    "    ax[0, 0].set_title(\"Model Training\",fontsize = 26)\n",
    "    ax[0, 1].set_title(\"Accuracy Score on Training Subset\",fontsize = 26)\n",
    "    ax[0, 2].set_title(\"F-beta(0.5) on Training Subset\",fontsize = 26)\n",
    "    ax[1, 0].set_title(\"Model Predicting\",fontsize = 26)\n",
    "    ax[1, 1].set_title(\"Accuracy Score on Testing Set\",fontsize = 26)\n",
    "    ax[1, 2].set_title(\"F-beta(0.5) on Testing Set\",fontsize = 26)\n",
    "    \n",
    "    # Add horizontal lines for naive predictors\n",
    "    ax[0, 1].axhline(y = accuracy, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n",
    "    ax[1, 1].axhline(y = accuracy, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n",
    "    ax[0, 2].axhline(y = f1, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n",
    "    ax[1, 2].axhline(y = f1, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n",
    "    \n",
    "    # Set y-limits for score panels\n",
    "    ax[0, 1].set_ylim((0, 1))\n",
    "    ax[0, 2].set_ylim((0, 1))\n",
    "    ax[1, 1].set_ylim((0, 1))\n",
    "    ax[1, 2].set_ylim((0, 1))\n",
    "\n",
    "    # Create patches for the legend\n",
    "    patches = []\n",
    "    for i, learner in enumerate(results.keys()):\n",
    "        patches.append(mpatches.Patch(color = colors[i], label = learner))\n",
    "    plt.legend(handles = patches,bbox_to_anchor=(0.5, 1,0.5,0.5), loc='upper center',ncol = 3, fontsize = 26)\n",
    "    \n",
    "    # Aesthetics\n",
    "    #plt.figlegend( 'Ruchin', 'Patel', loc = 'lower center', ncol=5, labelspacing=0. )\n",
    "    plt.suptitle(\"Performance Metrics for 5 Supervised Learning Models\", fontsize = 26, y = 1.10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "        \n",
    "def plot_densityCurve(*args):\n",
    "    plt.figure(figsize=(9,4), dpi=300)\n",
    "    sns.distplot(args[0],kde_kws=args[1])\n",
    "    plt.axvline(args[5], color='yellow', linestyle='-.', linewidth=1,label='sample mean')\n",
    "    plt.axvline(args[5]-args[6], color='black', linestyle=':', linewidth=1,label='1 standard dev')\n",
    "    plt.axvline(args[5]+args[6], color='black', linestyle=':', linewidth=1)\n",
    "    plt.axvline(args[7], color='purple', linestyle='-.', linewidth=2,label='True mean')\n",
    "    plt.axvline(args[5]-(1.96*args[6]),color='black',linewidth=2,label='95% confidence line')\n",
    "    plt.axvline(args[5]+(1.96*args[6]),color='black',linewidth=2)\n",
    "    #plt.xlim(0.72,0.85)\n",
    "    plt.legend()\n",
    "    #plt.title(\"The sampling distribution with \"+str(args[3])+\" samples of size n=\"+str(args[4]))\n",
    "    plt.title(args[8])\n",
    "    plt.show()  \n",
    "    \n",
    "def plot_norm(sample_mean,SE,*args):\n",
    "    plt.figure(figsize=(9,4), dpi=300)\n",
    "    x_values = np.random.normal(loc = sample_mean,scale=SE,size=args[0])\n",
    "    x_values = np.sort(x_values)\n",
    "    y_values = st.norm.pdf(x_values,loc = sample_mean,scale=SE)\n",
    "    plt.plot(x_values,y_values,linewidth=2,color='green')\n",
    "    plt.axvline(sample_mean, color='yellow', linestyle='-.', linewidth=2,label='sample mean')\n",
    "    plt.axvline(sample_mean-SE, color='black', linestyle=':', linewidth=1,label='1 standard dev')\n",
    "    plt.axvline(sample_mean+SE, color='black', linestyle=':', linewidth=1)\n",
    "    plt.axvline(args[1], color='purple', linestyle='-', linewidth=1,label='95% confidence line')\n",
    "    plt.axvline(args[2], color='purple', linestyle='-', linewidth=1)\n",
    "    x_95 = x_values[np.logical_and(x_values>=args[1],x_values<=args[2])]\n",
    "    y_95 = y_values[np.logical_and(x_values>=args[1],x_values<=args[2])]\n",
    "    plt.fill_between(x_95,0,y_95,color='red',alpha=0.4)\n",
    "    #plt.ylim(0,10)\n",
    "    plt.legend()\n",
    "    plt.show()  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
